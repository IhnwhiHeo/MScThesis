
@online{PDFEquivalenceTests,
  langid = {english},
  title = {(1) ({{PDF}}) {{Equivalence}} Tests – {{A}} Review},
  url = {https://www.researchgate.net/publication/257390208_Equivalence_tests_-_A_review},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  journaltitle = {ResearchGate},
  urldate = {2019-06-05},
  file = {C\:\\Users\\User\\Zotero\\storage\\4SVB2ENI\\(1) (PDF) Equivalence tests – A review.html}
}

@article{lakensEquivalenceTestsPractical2017,
  langid = {english},
  title = {Equivalence {{Tests}}: {{A Practical Primer}} for t {{Tests}}, {{Correlations}}, and {{Meta}}-{{Analyses}}},
  volume = {8},
  issn = {1948-5506},
  doi = {10.1177/1948550617697177},
  shorttitle = {Equivalence {{Tests}}},
  abstract = {Scientists should be able to provide support for the absence of a meaningful effect. Currently, researchers often incorrectly conclude an effect is absent based a nonsignificant result. A widely recommended approach within a frequentist framework is to test for equivalence. In equivalence tests, such as the two one-sided tests (TOST) procedure discussed in this article, an upper and lower equivalence bound is specified based on the smallest effect size of interest. The TOST procedure can be used to statistically reject the presence of effects large enough to be considered worthwhile. This practical primer with accompanying spreadsheet and R package enables psychologists to easily perform equivalence tests (and power analyses) by setting equivalence bounds based on standardized effect sizes and provides recommendations to prespecify equivalence bounds. Extending your statistical tool kit with equivalence tests is an easy way to improve your statistical and theoretical inferences.},
  number = {4},
  journaltitle = {Social Psychological and Personality Science},
  shortjournal = {Social Psychological and Personality Science},
  date = {2017-05-01},
  pages = {355-362},
  author = {Lakens, Daniël},
  file = {C\:\\Users\\User\\Zotero\\storage\\WVRV3QIE\\Lakens - 2017 - Equivalence Tests A Practical Primer for t Tests,.pdf}
}

@online{BorensteinHedgesHiggins,
  title = {Borenstein, Hedges, Higgins, \& Rothstein - Introduction to Meta-Analysis.Pdf},
  url = {https://drive.google.com/a/students.uu.nl/file/d/1ok_vPvjRXlXlFkc0vNCdf0Su64kfHfMd/view?usp=drive_open&usp=embed_facebook},
  journaltitle = {Google Docs},
  urldate = {2019-06-05},
  file = {C\:\\Users\\User\\Zotero\\storage\\ZFREFRTJ\\borenstein, hedges, higgins, & rothstein - introdu.html}
}

@article{meynersEquivalenceTestsReview2012,
  langid = {english},
  title = {Equivalence Tests – {{A}} Review},
  volume = {26},
  issn = {09503293},
  doi = {10.1016/j.foodqual.2012.05.003},
  number = {2},
  journaltitle = {Food Quality and Preference},
  shortjournal = {Food Quality and Preference},
  date = {2012-12},
  pages = {231-245},
  author = {Meyners, Michael},
  file = {C\:\\Users\\User\\Zotero\\storage\\XFJJ4CPE\\Meyners - 2012 - Equivalence tests – A review.pdf}
}

@online{PREPRINTLakensEtal,
  title = {{{PREPRINT}}\_{{Lakens}}\_etal\_{{EquivalenceTestingTutorial}}\_20171117.Pdf},
  url = {https://drive.google.com/a/students.uu.nl/file/d/19vPmFODghUYR145qj4EXOAlD6iY_mJNM/view?usp=drive_open&usp=embed_facebook},
  journaltitle = {Google Docs},
  urldate = {2019-06-05},
  file = {C\:\\Users\\User\\Zotero\\storage\\DZJ3XJCT\\PREPRINT_Lakens_etal_EquivalenceTestingTutorial_20.pdf;C\:\\Users\\User\\Zotero\\storage\\6T28RXLL\\view.html}
}

@article{huedo-medinaAssessingHeterogeneityMetaanalysis2006,
  langid = {english},
  title = {Assessing Heterogeneity in Meta-Analysis: {{Q}} Statistic or {{I}}² Index?},
  volume = {11},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.11.2.193},
  shorttitle = {Assessing Heterogeneity in Meta-Analysis},
  abstract = {In meta-analysis, the usual way of assessing whether a set of single studies are homogeneous is by means of the Q test. However, the Q test only informs us about the presence versus the absence of heterogeneity, but it does not report on the extent of such heterogeneity. Recently, the I2 index has been proposed to quantify the degree of heterogeneity in a meta-analysis. In this paper, the performances of the Q test and the confidence interval around the I2 index are compared by means of a Monte Carlo simulation. The results show the utility of the I2 index as a complement to the Q test, although it has the same problems of power with a small number of studies.},
  number = {2},
  journaltitle = {Psychological Methods},
  shortjournal = {Psychological Methods},
  date = {2006},
  pages = {193-206},
  author = {Huedo-Medina, Tania B. and Sánchez-Meca, Julio and Marín-Martínez, Fulgencio and Botella, Juan},
  file = {C\:\\Users\\User\\Zotero\\storage\\AWAQ8LAZ\\Huedo-Medina e.a. - 2006 - Assessing heterogeneity in meta-analysis Q statis.pdf}
}

@article{EquivalenceTestingPsychological,
  langid = {english},
  title = {Equivalence {{Testing}} for {{Psychological Research}}: {{A Tutorial}}},
  pages = {28},
  file = {C\:\\Users\\User\\Zotero\\storage\\NGADHTNZ\\Equivalence Testing for Psychological Research A .pdf}
}

@collection{borensteinIntroductionMetaanalysis2009,
  langid = {english},
  location = {{Chichester, U.K}},
  title = {Introduction to Meta-Analysis},
  isbn = {978-0-470-05724-7},
  abstract = {This text provides a concise and clearly presented discussion of all the elements in a meta-analysis. It is illustrated with worked examples throughout, with visual explanations, using screenshots from Excel spreadsheets and computer programs such as Comprehensive Meta-Analysis (CMA) or Strata},
  pagetotal = {421},
  publisher = {{John Wiley \& Sons}},
  date = {2009},
  keywords = {Meta-analysis,Meta-Analysis as Topic},
  editor = {Borenstein, Michael},
  file = {C\:\\Users\\User\\Zotero\\storage\\WLFD7PSX\\Borenstein - 2009 - Introduction to meta-analysis.pdf},
  note = {OCLC: ocn263294996}
}

@online{QuantifyingHeterogeneityMeta,
  title = {Quantifying Heterogeneity in a Meta‐analysis - {{Higgins}} - 2002 - {{Statistics}} in {{Medicine}} - {{Wiley Online Library}}},
  url = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.1186},
  urldate = {2019-06-13}
}

@article{higginsMeasuringInconsistencyMetaanalyses2003,
  title = {Measuring Inconsistency in Meta-Analyses},
  volume = {327},
  issn = {0959-8138},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC192859/},
  abstract = {Cochrane Reviews have recently started including the quantity I2 to help readers assess the consistency of the results of studies in meta-analyses. What does this new quantity mean, and why is assessment of heterogeneity so important to clinical practice?},
  number = {7414},
  journaltitle = {BMJ : British Medical Journal},
  shortjournal = {BMJ},
  urldate = {2019-06-13},
  date = {2003-09-06},
  pages = {557-560},
  author = {Higgins, Julian P T and Thompson, Simon G and Deeks, Jonathan J and Altman, Douglas G},
  file = {C\:\\Users\\User\\Zotero\\storage\\V75XI7ZS\\Higgins et al. - 2003 - Measuring inconsistency in meta-analyses.pdf},
  eprinttype = {pmid},
  eprint = {12958120},
  pmcid = {PMC192859}
}

@article{campbellCanWeDisregard2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.11875},
  primaryClass = {stat},
  title = {Can We Disregard the Whole Model? {{Omnibus}} Non-Inferiority Testing for \${{R}}\^\{2\}\$ in Multivariable Linear Regression and \$\textbackslash{}hat\{\textbackslash{}eta\}\^\{2\}\$ in {{ANOVA}}},
  url = {http://arxiv.org/abs/1905.11875},
  shorttitle = {Can We Disregard the Whole Model?},
  abstract = {Determining a lack of association between an outcome variable and a number of different explanatory variables is frequently necessary in order to disregard a proposed model (i.e., to confirm the lack of an association between an outcome and predictors). Despite this, the literature rarely offers information about, or technical recommendations concerning, the appropriate statistical methodology to be used to accomplish this task. This paper introduces non-inferiority tests for ANOVA and linear regression analyses, that correspond to the standard widely used \$F\$-test for \$\textbackslash{}hat\{\textbackslash{}eta\}\^2\$ and \$R\^\{2\}\$, respectively. A simulation study is conducted to examine the type I error rates and statistical power of the tests, and a comparison is made with an alternative Bayesian testing approach. The results indicate that the proposed non-inferiority test is a potentially useful tool for 'testing the null.'},
  urldate = {2019-06-13},
  date = {2019-05-28},
  keywords = {Statistics - Applications,Statistics - Methodology},
  author = {Campbell, Harlan and Lakens, Daniël}
}

@book{cohenStatisticalPowerAnalysis1988,
  langid = {english},
  location = {{Hillsdale, N.J}},
  title = {Statistical Power Analysis for the Behavioral Sciences},
  edition = {2nd ed},
  isbn = {978-0-8058-0283-2},
  pagetotal = {567},
  publisher = {{L. Erlbaum Associates}},
  date = {1988},
  keywords = {Probabilities,Social sciences,Statistical methods,Statistical power analysis},
  author = {Cohen, Jacob},
  file = {C\:\\Users\\User\\Zotero\\storage\\5YXGF8P9\\Cohen - 1988 - Statistical power analysis for the behavioral scie.pdf}
}

@article{faulPowerFlexibleStatistical2007,
  langid = {english},
  title = {G*{{Power}} 3: {{A}} Flexible Statistical Power Analysis Program for the Social, Behavioral, and Biomedical Sciences},
  volume = {39},
  issn = {1554-351X, 1554-3528},
  doi = {10.3758/BF03193146},
  shorttitle = {G*{{Power}} 3},
  number = {2},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behavior Research Methods},
  date = {2007-05},
  pages = {175-191},
  author = {Faul, Franz and Erdfelder, Edgar and Lang, Albert-Georg and Buchner, Axel},
  file = {C\:\\Users\\User\\Zotero\\storage\\Z7XWPRKC\\Faul e.a. - 2007 - GPower 3 A flexible statistical power analysis p.pdf}
}

@book{cummingUnderstandingNewStatistics2013,
  langid = {english},
  title = {Understanding {{The New Statistics}}: {{Effect Sizes}}, {{Confidence Intervals}}, and {{Meta}}-{{Analysis}}},
  edition = {1},
  isbn = {978-0-203-80700-2},
  url = {https://www.taylorfrancis.com/books/9780203807002},
  shorttitle = {Understanding {{The New Statistics}}},
  publisher = {{Routledge}},
  urldate = {2019-06-13},
  date = {2013-06-19},
  author = {Cumming, Geoff},
  file = {C\:\\Users\\User\\Zotero\\storage\\8XCE5YCB\\Cumming - 2013 - Understanding The New Statistics Effect Sizes, Co.pdf},
  doi = {10.4324/9780203807002}
}

@book{wellekTestingStatisticalHypotheses2010,
  langid = {english},
  title = {Testing {{Statistical Hypotheses}} of {{Equivalence}} and {{Noninferiority}}},
  edition = {0},
  isbn = {978-0-429-09267-1},
  url = {https://www.taylorfrancis.com/books/9781439808191},
  publisher = {{Chapman and Hall/CRC}},
  urldate = {2019-06-13},
  date = {2010-06-24},
  author = {Wellek, Stefan},
  file = {C\:\\Users\\User\\Zotero\\storage\\QL94TD8X\\Wellek - 2010 - Testing Statistical Hypotheses of Equivalence and .pdf},
  doi = {10.1201/EBK1439808184}
}

@online{NoncentralityParameterOverview,
  title = {Noncentrality Parameter - an Overview | {{ScienceDirect Topics}}},
  url = {https://www.sciencedirect.com/topics/mathematics/noncentrality-parameter},
  urldate = {2019-06-13},
  file = {C\:\\Users\\User\\Zotero\\storage\\JTWNXEPU\\noncentrality-parameter.html}
}

@article{hewittNoteComputingChisquare1988,
  langid = {english},
  title = {A Note on Computing the Chi-Square Noncentrality Parameter for Power Analyses},
  volume = {18},
  issn = {1573-3297},
  doi = {10.1007/BF01067079},
  abstract = {Power calculations for a variety of research designs used in behavior genetics require the determination of a chi-square noncentrality parameter. For many purposes published tables are adequate, but for cases where they are not we provide a graph for approximating the required parameters for up to 300 degrees of freedom and illustrate a straightforward procedure for accurate determination using a widely available SAS function.},
  number = {1},
  journaltitle = {Behavior Genetics},
  shortjournal = {Behav Genet},
  date = {1988-01-01},
  pages = {105-108},
  keywords = {noncentral chi-square,power calculation},
  author = {Hewitt, J. K. and Heath, A. C.},
  file = {C\:\\Users\\User\\Zotero\\storage\\YLS25CMC\\Hewitt en Heath - 1988 - A note on computing the chi-square noncentrality p.pdf}
}

@book{dingAlgorithm275Computing1990,
  title = {Algorithm {{AS}} 275 {{Computing}} the {{Non}}-Central X2 {{Distribution Function}}},
  abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. Wiley and Royal Statistical Society are collaborating with JSTOR to digitize, preserve and extend access to},
  date = {1990},
  author = {Ding, G. and Col, Pos and Dingt, G.},
  file = {C\:\\Users\\User\\Zotero\\storage\\IC5UU37L\\Ding e.a. - 1990 - Algorithm AS 275 Computing the Non-central x2 Dist.pdf;C\:\\Users\\User\\Zotero\\storage\\KGISLKS9\\summary.html}
}

@book{pigottAdvancesMetaAnalysis2012,
  langid = {english},
  title = {Advances in {{Meta}}-{{Analysis}}},
  isbn = {978-1-4614-2277-8},
  abstract = {The subject of the book is advanced statistical analyses for quantitative research synthesis (meta-analysis), and selected practical issues relating to research synthesis that are not covered in detail in the many existing introductory books on research synthesis (or meta-analysis). Complex statistical issues are arising more frequently as the primary research that is summarized in quantitative syntheses itself becomes more complex, and as researchers who are conducting meta-analyses become more ambitious in the questions they wish to address. Also as researchers have gained more experience in conducting research syntheses, several key issues have persisted and now appear fundamental to the enterprise of summarizing research.Specifically the book describes multivariate analyses for several indices commonly used in meta-analysis (e.g., correlations, effect sizes, proportions and/or odds ratios), will outline how to do power analysis for meta-analysis (again for each of the different kinds of study outcome indices), and examines issues around research quality and research design and their roles in synthesis. For each of the statistical topics we will examine the different possible statistical models (i.e., fixed, random, and mixed models) that could be adopted by a researcher. In dealing with the issues of study quality and research design it covers a number of specific topics that are of broad concern to research synthesists. In many fields a current issue is how to make sense of results when studies using several different designs appear in a research literature (e.g., Morris \& Deshon, 1997, 2002). In education and other social sciences a critical aspect of this issue is how one might incorporate qualitative (e.g., case study) research within a synthesis. In medicine, related issues concern whether and how to summarize observational studies, and whether they should be combined with randomized controlled trials (or even if they should be combined at all). For each topic, included is a worked example (e.g., for the statistical analyses) and/or a detailed description of a published research synthesis that deals with the practical (non-statistical) issues covered.},
  pagetotal = {166},
  publisher = {{Springer Science \& Business Media}},
  date = {2012-01-31},
  keywords = {Mathematics / Probability & Statistics / General,Social Science / Research,Social Science / Statistics},
  author = {Pigott, Terri},
  eprinttype = {googlebooks}
}

@article{yuShapeNoncentralChisquare2011,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1106.5241},
  primaryClass = {math, stat},
  title = {The {{Shape}} of the {{Noncentral Chi}}-Square {{Density}}},
  url = {http://arxiv.org/abs/1106.5241},
  abstract = {A noncentral chi-square density is log-concave if the degree of freedom is nu{$>$}=2. We complement this known result by showing that, for each 0{$<$}nu{$<$}2, there exists lambda\_nu{$>$}0 such that the chi-square with nu degrees of freedom and noncentrality parameter lambda has a decreasing density if lambda {$<$}= lambda\_nu, and is bi-modal otherwise. The critical lambda\_nu is characterized by an equation involving a ratio of modified Bessel functions. When an interior mode exists we derive precise bounds on its location.},
  urldate = {2019-06-13},
  date = {2011-06-26},
  keywords = {60E05; 62E15; 33C10,Mathematics - Classical Analysis and ODEs,Mathematics - Statistics Theory},
  author = {Yu, Yaming},
  file = {C\:\\Users\\User\\Zotero\\storage\\VRI965FJ\\Yu - 2011 - The Shape of the Noncentral Chi-square Density.pdf;C\:\\Users\\User\\Zotero\\storage\\FYLPLIK5\\1106.html}
}

@article{yuShapeNoncentralChisquare2011a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1106.5241},
  primaryClass = {math, stat},
  title = {The {{Shape}} of the {{Noncentral Chi}}-Square {{Density}}},
  url = {http://arxiv.org/abs/1106.5241},
  abstract = {A noncentral chi-square density is log-concave if the degree of freedom is nu{$>$}=2. We complement this known result by showing that, for each 0{$<$}nu{$<$}2, there exists lambda\_nu{$>$}0 such that the chi-square with nu degrees of freedom and noncentrality parameter lambda has a decreasing density if lambda {$<$}= lambda\_nu, and is bi-modal otherwise. The critical lambda\_nu is characterized by an equation involving a ratio of modified Bessel functions. When an interior mode exists we derive precise bounds on its location.},
  urldate = {2019-06-17},
  date = {2011-06-26},
  keywords = {60E05; 62E15; 33C10,Mathematics - Classical Analysis and ODEs,Mathematics - Statistics Theory},
  author = {Yu, Yaming}
}

@article{cummingPrimerUnderstandingUse,
  langid = {english},
  title = {A {{Primer}} on the {{Understanding}}, {{Use}}, and {{Calculation}} of {{Confidence Intervals}} That Are {{Based}} on {{Central}} and {{Noncentral Distributions}}},
  journaltitle = {EDUCATIONAL AND PSYCHOLOGICAL MEASUREMENT},
  pages = {43},
  author = {Cumming, Geoff and Finch, Sue},
  file = {C\:\\Users\\User\\Zotero\\storage\\A9YDW324\\Cumming en Finch - A Primer on the Understanding, Use, and Calculatio.pdf}
}

@article{guoTestingTwoVariances,
  langid = {english},
  title = {Testing Two Variances for Superiority/Non-Inferiority and Equivalence: {{Using}} the Exhaustion Algorithm for Sample Size Allocation with Cost},
  volume = {0},
  issn = {2044-8317},
  doi = {10.1111/bmsp.12172},
  shorttitle = {Testing Two Variances for Superiority/Non-Inferiority and Equivalence},
  abstract = {The equality of two group variances is frequently tested in experiments. However, criticisms of null hypothesis statistical testing on means have recently arisen and there is interest in other types of statistical tests of hypotheses, such as superiority/non-inferiority and equivalence. Although these tests have become more common in psychology and social sciences, the corresponding sample size estimation for these tests is rarely discussed, especially when the sampling unit costs are unequal or group sizes are unequal for two groups. Thus, for finding optimal sample size, the present study derived an initial allocation by approximating the percentiles of an F distribution with the percentiles of the standard normal distribution and used the exhaustion algorithm to select the best combination of group sizes, thereby ensuring the resulting power reaches the designated level and is maximal with a minimal total cost. In this manner, optimization of sample size planning is achieved. The proposed sample size determination has a wide range of applications and is efficient in terms of Type I errors and statistical power in simulations. Finally, an illustrative example from a report by the Health Survey for England, 1995–1997, is presented using hypertension data. For ease of application, four R Shiny apps are provided and benchmarks for setting equivalence margins are suggested.},
  number = {0},
  journaltitle = {British Journal of Mathematical and Statistical Psychology},
  keywords = {hypertension data,optimal allocation,power analysis,sampling cost,Snedecor's F test,variance ratio},
  author = {Guo, Jiin-huarng and Luh, Wei-ming},
  file = {C\:\\Users\\User\\Zotero\\storage\\LVZVTMCR\\Guo en Luh - Testing two variances for superioritynon-inferior.pdf;C\:\\Users\\User\\Zotero\\storage\\KN8JM3YP\\bmsp.html}
}

@article{campbellCanWeDisregard2019a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.11875},
  primaryClass = {stat},
  title = {Can We Disregard the Whole Model? {{Omnibus}} Non-Inferiority Testing for \${{R}}\^\{2\}\$ in Multivariable Linear Regression and \$\textbackslash{}hat\{\textbackslash{}eta\}\^\{2\}\$ in {{ANOVA}}},
  url = {http://arxiv.org/abs/1905.11875},
  shorttitle = {Can We Disregard the Whole Model?},
  abstract = {Determining a lack of association between an outcome variable and a number of different explanatory variables is frequently necessary in order to disregard a proposed model (i.e., to confirm the lack of an association between an outcome and predictors). Despite this, the literature rarely offers information about, or technical recommendations concerning, the appropriate statistical methodology to be used to accomplish this task. This paper introduces non-inferiority tests for ANOVA and linear regression analyses, that correspond to the standard widely used \$F\$-test for \$\textbackslash{}hat\{\textbackslash{}eta\}\^2\$ and \$R\^\{2\}\$, respectively. A simulation study is conducted to examine the type I error rates and statistical power of the tests, and a comparison is made with an alternative Bayesian testing approach. The results indicate that the proposed non-inferiority test is a potentially useful tool for 'testing the null.'},
  urldate = {2019-06-19},
  date = {2019-05-28},
  keywords = {Statistics - Applications,Statistics - Methodology},
  author = {Campbell, Harlan and Lakens, Daniël},
  file = {C\:\\Users\\User\\Zotero\\storage\\VE9KSLG4\\Campbell en Lakens - 2019 - Can we disregard the whole model Omnibus non-infe.pdf;C\:\\Users\\User\\Zotero\\storage\\65D7IHLD\\1905.html}
}

@article{kelleyConfidenceIntervalsStandardized2007,
  langid = {english},
  title = {Confidence {{Intervals}} for {{Standardized Effect Sizes}}: {{Theory}}, {{Application}}, and {{Implementation}}},
  volume = {20},
  issn = {1548-7660},
  doi = {10.18637/jss.v020.i08},
  shorttitle = {Confidence {{Intervals}} for {{Standardized Effect Sizes}}},
  number = {1},
  journaltitle = {Journal of Statistical Software},
  date = {2007-02-22},
  pages = {1-24},
  author = {Kelley, Ken},
  file = {C\:\\Users\\User\\Zotero\\storage\\MCEE7LJB\\Kelley - 2007 - Confidence Intervals for Standardized Effect Sizes.pdf;C\:\\Users\\User\\Zotero\\storage\\7KHZJRI8\\v020i08.html}
}

@article{janSampleSizeDeterminations2014,
  langid = {english},
  title = {Sample Size Determinations for {{Welch}}'s Test in One-Way Heteroscedastic {{ANOVA}}},
  volume = {67},
  issn = {2044-8317},
  doi = {10.1111/bmsp.12006},
  abstract = {For one-way fixed effects ANOVA, it is well known that the conventional F test of the equality of means is not robust to unequal variances, and numerous methods have been proposed for dealing with heteroscedasticity. On the basis of extensive empirical evidence of Type I error control and power performance, Welch's procedure is frequently recommended as the major alternative to the ANOVA F test under variance heterogeneity. To enhance its practical usefulness, this paper considers an important aspect of Welch's method in determining the sample size necessary to achieve a given power. Simulation studies are conducted to compare two approximate power functions of Welch's test for their accuracy in sample size calculations over a wide variety of model configurations with heteroscedastic structures. The numerical investigations show that Levy's (1978a) approach is clearly more accurate than the formula of Luh and Guo (2011) for the range of model specifications considered here. Accordingly, computer programs are provided to implement the technique recommended by Levy for power calculation and sample size determination within the context of the one-way heteroscedastic ANOVA model.},
  number = {1},
  journaltitle = {The British Journal of Mathematical and Statistical Psychology},
  shortjournal = {Br J Math Stat Psychol},
  date = {2014-02},
  pages = {72-93},
  keywords = {Analysis of Variance,Cost Allocation,Humans,Models; Statistical,Research Design,Sample Size,Software},
  author = {Jan, Show-Li and Shieh, Gwowen},
  eprinttype = {pmid},
  eprint = {23316952}
}

@online{SampleSizeDeterminations,
  title = {Sample Size Determinations for {{Welch}}'s Test in One‐way Heteroscedastic {{ANOVA}} - {{Jan}} - 2014 - {{British Journal}} of {{Mathematical}} and {{Statistical Psychology}} - {{Wiley Online Library}}},
  url = {https://onlinelibrary.wiley.com/doi/full/10.1111/bmsp.12006?sid=nlm%3Apubmed},
  urldate = {2019-06-19},
  file = {C\:\\Users\\User\\Zotero\\storage\\WFME75Q6\\bmsp.html}
}

@article{curranNoncentralChisquareDistribution2002,
  title = {The {{Noncentral Chi}}-Square {{Distribution}} in {{Misspecified Structural Equation Models}}: {{Finite Sample Results}} from a {{Monte Carlo Simulation}}},
  volume = {37},
  issn = {0027-3171},
  doi = {10.1207/S15327906MBR3701_01},
  shorttitle = {The {{Noncentral Chi}}-Square {{Distribution}} in {{Misspecified Structural Equation Models}}},
  abstract = {The noncentral chi-square distribution plays a key role in structural equation modeling (SEM). The likelihood ratio test statistic that accompanies virtually all SEMs asymptotically follows a noncentral chi-square under certain assumptions relating to misspecification and multivariate distribution. Many scholars use the noncentral chi-square distribution in the construction of fit indices, such as Steiger and Lind's (1980) Root Mean Square Error of Approximation (RMSEA) or the family of baseline fit indices (e.g., RNI, CFI), and for the computation of statistical power for model hypothesis testing. Despite this wide use, surprisingly little is known about the extent to which the test statistic follows a noncentral chi-square in applied research. Our study examines several hypotheses about the suitability of the noncentral chi-square distribution for the usual SEM test statistic under conditions commonly encountered in practice. We designed Monte Carlo computer simulation experiments to empirically test these research hypotheses. Our experimental conditions included seven sample sizes ranging from 50 to 1000, and three distinct model types, each with five specifications ranging from a correct model to the severely misspecified uncorrelated baseline model. In general, we found that for models with small to moderate misspecification, the noncentral chi-square distribution is well approximated when the sample size is large (e.g., greater than 200), but there was evidence of bias in both mean and variance in smaller samples. A key finding was that the test statistics for the uncorrelated variable baseline model did not follow the noncentral chi-square distribution for any model type across any sample size. We discuss the implications of our findings for the SEM fit indices and power estimation procedures that are based on the noncentral chi-square distribution as well as potential directions for future research.},
  number = {1},
  journaltitle = {Multivariate Behavioral Research},
  date = {2002-01-01},
  pages = {1-36},
  author = {Curran, Patrick J. and Bollen, Kenneth A. and Paxton, Pamela and Kirby, James and Chen, Feinian},
  file = {C\:\\Users\\User\\Zotero\\storage\\XX7WKIXD\\Curran e.a. - 2002 - The Noncentral Chi-square Distribution in Misspeci.pdf;C\:\\Users\\User\\Zotero\\storage\\DDRLBYFI\\S15327906MBR3701_01.html},
  eprinttype = {pmid},
  eprint = {26824167}
}

@article{kelleyConfidenceIntervalsStandardized2007a,
  langid = {english},
  title = {Confidence {{Intervals}} for {{Standardized Effect Sizes}}: {{Theory}}, {{Application}}, and {{Implementation}}},
  volume = {20},
  issn = {1548-7660},
  doi = {10.18637/jss.v020.i08},
  shorttitle = {Confidence {{Intervals}} for {{Standardized Effect Sizes}}},
  number = {1},
  journaltitle = {Journal of Statistical Software},
  date = {2007-02-22},
  pages = {1-24},
  author = {Kelley, Ken},
  file = {C\:\\Users\\User\\Zotero\\storage\\QDZH3HNW\\Kelley - 2007 - Confidence Intervals for Standardized Effect Sizes.pdf;C\:\\Users\\User\\Zotero\\storage\\IZAQCCDB\\v020i08.html}
}

@article{janSampleSizeDeterminations2014a,
  title = {Sample Size Determinations for {{Welch}}'s Test in One-Way Heteroscedastic {{ANOVA}}},
  volume = {67},
  issn = {0007-1102},
  doi = {10.1111/bmsp.12006},
  abstract = {For one-way fixed effects ANOVA, it is well known that the conventional F test of the equality of means is not robust to unequal variances, and numerous methods have been proposed for dealing with heteroscedasticity. On the basis of extensive empirical evidence of Type I error control and power performance, Welch's procedure is frequently recommended as the major alternative to the ANOVA F test under variance heterogeneity. To enhance its practical usefulness, this paper considers an important aspect of Welch's method in determining the sample size necessary to achieve a given power. Simulation studies are conducted to compare two approximate power functions of Welch's test for their accuracy in sample size calculations over a wide variety of model configurations with heteroscedastic structures. The numerical investigations show that Levy's (1978a) approach is clearly more accurate than the formula of Luh and Guo (2011) for the range of model specifications considered here. Accordingly, computer programs are provided to implement the technique recommended by Levy for power calculation and sample size determination within the context of the one-way heteroscedastic ANOVA model.},
  number = {1},
  journaltitle = {British Journal of Mathematical and Statistical Psychology},
  shortjournal = {British Journal of Mathematical and Statistical Psychology},
  date = {2014-02-01},
  pages = {72-93},
  author = {Jan, Show-Li and Shieh, Gwowen},
  file = {C\:\\Users\\User\\Zotero\\storage\\V2S7GYC2\\Jan en Shieh - 2014 - Sample size determinations for Welch's test in one.pdf;C\:\\Users\\User\\Zotero\\storage\\B5E3ASHB\\bmsp.html}
}

@article{vinknd,
  langid = {english},
  title = {Towards a Standardized Evaluation of Multiple Imputation Routines},
  abstract = {Developing new imputation methodology has become a very active ﬁeld. Unfortunately, there is no consensus on how to perform simulation studies to evaluate the properties of imputation methods. In this paper I propose a move towards a standardized evaluation of imputation methods. To demonstrate the need for standardization, I highlight a set of potential pitfalls that bring forth a chain of potential problems in the objective assessment of the performance of imputation routines. This may lead to suboptimal use of multiple imputation in practice. Additionally, I suggest a course of action for simulating and evaluating missing data problems.},
  author = {Vink, Gerko},
  file = {C\:\\Users\\User\\Zotero\\storage\\ZXGCXPP4\\Vink - Towards a standardized evaluation of multiple impu.pdf}
}

@article{abay08,
  langid = {english},
  title = {Diagnostics for Multivariate Imputations},
  volume = {57},
  issn = {0035-9254, 1467-9876},
  doi = {10.1111/j.1467-9876.2007.00613.x},
  abstract = {We consider three sorts of diagnostics for random imputations: displays of the completed data, which are intended to reveal unusual patterns that might suggest problems with the imputations, comparisons of the distributions of observed and imputed data values and checks of the ﬁt of observed data to the model that is used to create the imputations. We formulate these methods in terms of sequential regression multivariate imputation, which is an iterative procedure in which the missing values of each variable are randomly imputed conditionally on all the other variables in the completed data matrix. We also consider a recalibration procedure for sequential regression imputations. We apply these methods to the 2002 environmental sustainability index, which is a linear aggregation of 64 environmental variables on 142 countries.},
  number = {3},
  journaltitle = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  shortjournal = {J Royal Statistical Soc C},
  date = {2008-06},
  pages = {273-291},
  author = {Abayomi, Kobi and Gelman, Andrew and Levy, Marc},
  file = {C\:\\Users\\User\\Zotero\\storage\\YHCKE6CH\\Abayomi e.a. - 2008 - Diagnostics for multivariate imputations.pdf}
}

@book{rubin87,
  langid = {english},
  location = {{New York, NY}},
  title = {Multiple {{Imputation}} for Nonresponse in Surveys},
  isbn = {978-0-471-08705-2},
  pagetotal = {258},
  series = {Wiley Series in Probability and Mathematical Statistics {{Applied}} Probability and Statistics},
  publisher = {{Wiley}},
  date = {1987},
  author = {Rubin, Donald B.},
  file = {C\:\\Users\\User\\Zotero\\storage\\63UKR3PY\\Rubin - 1987 - Multiple Imputation for nonresponse in surveys.pdf}
}

@article{rubin96,
  eprinttype = {jstor},
  eprint = {2291635},
  title = {Multiple {{Imputation After}} 18+ {{Years}}},
  volume = {91},
  issn = {01621459},
  doi = {10.2307/2291635},
  abstract = {[Multiple imputation was designed to handle the problem of missing data in public-use data bases where the data-base constructor and the ultimate user are distinct entities. The objective is valid frequency inference for ultimate users who in general have access only to complete-data software and possess limited knowledge of specific reasons and models for nonresponse. For this situation and objective, I believe that multiple imputation by the data-base constructor is the method of choice. This article first provides a description of the assumed context and objectives, and second, reviews the multiple imputation framework and its standard results. These preliminary discussions are especially important because some recent commentaries on multiple imputation have reflected either misunderstandings of the practical objectives of multiple imputation or misunderstandings of fundamental theoretical results. Then, criticisms of multiple imputation are considered, and, finally, comparisons are made to alternative strategies.]},
  number = {434},
  journaltitle = {Journal of the American Statistical Association},
  date = {1996},
  pages = {473-489},
  author = {Rubin, Donald B.},
  file = {C\:\\Users\\User\\Zotero\\storage\\9UVT28MF\\Rubin - Multiple Imputation after 18+ years.pdf}
}

@article{li91,
  title = {Significance Levels from Repeated P-Values with Multiply-Imputed Data},
  issn = {1017-0405},
  journaltitle = {Statistica Sinica},
  shortjournal = {Statistica Sinica},
  date = {1991},
  pages = {65-92},
  author = {Li, Kim-Hung and Meng, Xiao-Li and Raghunathan, Trivellore E and Rubin, Donald B},
  file = {C\:\\Users\\User\\Zotero\\storage\\3KXYTRMF\\Li Meng Raghunathan Rubin - combining p values.pdf}
}

@article{bart15,
  langid = {english},
  title = {Multiple Imputation of Covariates by Fully Conditional Specification: {{Accommodating}} the Substantive Model},
  volume = {24},
  issn = {0962-2802},
  doi = {10.1177/0962280214521348},
  shorttitle = {Multiple Imputation of Covariates by Fully Conditional Specification},
  abstract = {Missing covariate data commonly occur in epidemiological and clinical research, and are often dealt with using multiple imputation. Imputation of partially observed covariates is complicated if the substantive model is non-linear (e.g. Cox proportional hazards model), or contains non-linear (e.g. squared) or interaction terms, and standard software implementations of multiple imputation may impute covariates from models that are incompatible with such substantive models. We show how imputation by fully conditional specification, a popular approach for performing multiple imputation, can be modified so that covariates are imputed from models which are compatible with the substantive model. We investigate through simulation the performance of this proposal, and compare it with existing approaches. Simulation results suggest our proposal gives consistent estimates for a range of common substantive models, including models which contain non-linear covariate effects or interactions, provided data are missing at random and the assumed imputation models are correctly specified and mutually compatible. Stata software implementing the approach is freely available.},
  number = {4},
  journaltitle = {Statistical Methods in Medical Research},
  shortjournal = {Stat Methods Med Res},
  date = {2015-08-01},
  pages = {462-487},
  author = {Bartlett, Jonathan W and Seaman, Shaun R and White, Ian R and Carpenter, James R},
  file = {C\:\\Users\\User\\Zotero\\storage\\IP6F7U98\\Bartlett e.a. - 2015 - Multiple imputation of covariates by fully conditi.pdf}
}

@book{buur18,
  title = {Flexible Imputation of Missing Data},
  isbn = {0-429-96035-2},
  publisher = {{Chapman and Hall/CRC}},
  date = {2018},
  author = {Van Buuren, Stef}
}

@software{shiny17,
  title = {Shiny: {{Web Application Framework}} for {{R}}},
  url = {https://CRAN.R-project.org/package=shiny},
  shorttitle = {Shiny},
  abstract = {Makes it incredibly easy to build interactive web applications with R. Automatic "reactive" binding between inputs and outputs and extensive prebuilt widgets make it possible to build beautiful, responsive, and powerful applications with minimal effort.},
  version = {1.3.2},
  urldate = {2019-09-05},
  date = {2019-04-22},
  keywords = {TeachingStatistics,WebTechnologies},
  author = {Chang, Winston and Cheng, Joe and Allaire, J. J. and Xie, Yihui and McPherson, Jonathan and RStudio and {library)}, jQuery Foundation (jQuery library {and} jQuery UI and library; authors listed in inst/www/shared/jquery- AUTHORS.txt), jQuery contributors (jQuery and library; authors listed in {inst/www/shared/jqueryui/AUTHORS.txt)}, jQuery UI contributors (jQuery UI and {library)}, Mark Otto (Bootstrap and {library)}, Jacob Thornton (Bootstrap and {library)}, Bootstrap contributors (Bootstrap and Twitter and {library)}, Inc (Bootstrap and {library)}, Alexander Farkas (html5shiv and js {library)}, Scott Jehl (Respond and {library)}, Stefan Petre (Bootstrap-datepicker and {library)}, Andrew Rowls (Bootstrap-datepicker and {font)}, Dave Gandy (Font-Awesome and js {library)}, Brian Reavis (selectize and {library)}, Kristopher Michael Kowal (es5-shim and {library)}, es5-shim contributors (es5-shim and rangeSlider {library)}, Denis Ineshin (ion and strftime {library)}, Sami Samhuri (Javascript and {library)}, SpryMedia Limited (DataTables and js {library)}, John Fraser (showdown and js {library)}, John Gruber (showdown and js {library)}, Ivan Sagalaev (highlight and implementation from R), R. Core Team (tar},
  options = {useprefix=true},
  note = {Add package version!}
}

@article{mice11,
  langid = {english},
  title = {Mice: {{Multivariate Imputation}} by {{Chained Equations}} in {{R}}},
  volume = {45},
  issn = {1548-7660},
  doi = {10.18637/jss.v045.i03},
  shorttitle = {Mice},
  number = {1},
  journaltitle = {Journal of Statistical Software},
  date = {2011-12-12},
  pages = {1-67},
  author = {Van Buuren, Stef and Groothuis-Oudshoorn, Karin},
  file = {C\:\\Users\\User\\Zotero\\storage\\W7DXJIVB\\Buuren en Groothuis-Oudshoorn - 2011 - mice Multivariate Imputation by Chained Equations.pdf;C\:\\Users\\User\\Zotero\\storage\\6GARXLG6\\v045i03.html}
}

@article{cowl96,
  title = {Markov Chain {{Monte Carlo}} Convergence Diagnostics: A Comparative Review},
  volume = {91},
  issn = {0162-1459},
  number = {434},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {Journal of the American Statistical Association},
  date = {1996},
  pages = {883-904},
  author = {Cowles, Mary Kathryn and Carlin, Bradley P},
  file = {C\:\\Users\\User\\Zotero\\storage\\JIBHQIG4\\CowlesCarlin.1996.pdf}
}

@article{scha02,
  title = {Missing Data: Our View of the State of the Art.},
  volume = {7},
  issn = {1939-1463},
  number = {2},
  journaltitle = {Psychological methods},
  shortjournal = {Psychological methods},
  date = {2002},
  pages = {147},
  author = {Schafer, Joseph L and Graham, John W},
  file = {C\:\\Users\\User\\Zotero\\storage\\RP4NJ9T2\\Schafer en Graham - 2002 - Missing data our view of the state of the art..pdf}
}

@article{whit11,
  langid = {english},
  title = {Multiple Imputation Using Chained Equations: {{Issues}} and Guidance for Practice},
  volume = {30},
  issn = {1097-0258},
  doi = {10.1002/sim.4067},
  shorttitle = {Multiple Imputation Using Chained Equations},
  abstract = {Multiple imputation by chained equations is a flexible and practical approach to handling missing data. We describe the principles of the method and show how to impute categorical and quantitative variables, including skewed variables. We give guidance on how to specify the imputation model and how many imputations are needed. We describe the practical analysis of multiply imputed data, including model building and model checking. We stress the limitations of the method and discuss the possible pitfalls. We illustrate the ideas using a data set in mental health, giving Stata code fragments.},
  number = {4},
  journaltitle = {Statistics in Medicine},
  shortjournal = {Stat Med},
  date = {2011-02-20},
  pages = {377-399},
  keywords = {Humans,Models; Statistical,Adolescent,Adult,Aged,Cardiovascular Diseases,Cholesterol,Female,Lipoproteins; HDL,Mental Health,Middle Aged,Multicenter Studies as Topic,Young Adult},
  author = {White, Ian R. and Royston, Patrick and Wood, Angela M.},
  file = {C\:\\Users\\User\\Zotero\\storage\\L74SBSX2\\White e.a. - 2011 - Multiple imputation using chained equations Issue.pdf},
  eprinttype = {pmid},
  eprint = {21225900}
}

@article{li14,
  title = {Multiple {{Imputation}} by {{Ordered Monotone Blocks With Application}} to the {{Anthrax Vaccine Research Program}}},
  volume = {23},
  issn = {1061-8600},
  doi = {10.1080/10618600.2013.826583},
  abstract = {Multiple imputation (MI) has become a standard statistical technique for dealing with missing values. The CDC Anthrax Vaccine Research Program (AVRP) dataset created new challenges for MI due to the large number of variables of different types and the limited sample size. A common method for imputing missing data in such complex studies is to specify, for each of J variables with missing values, a univariate conditional distribution given all other variables, and then to draw imputations by iterating over the J conditional distributions. Such fully conditional imputation strategies have the theoretical drawback that the conditional distributions may be incompatible. When the missingness pattern is monotone, a theoretically valid approach is to specify, for each variable with missing values, a conditional distribution given the variables with fewer or the same number of missing values and sequentially draw from these distributions. In this article, we propose the “multiple imputation by ordered monotone blocks” approach, which combines these two basic approaches by decomposing any missingness pattern into a collection of smaller “constructed” monotone missingness patterns, and iterating. We apply this strategy to impute the missing data in the AVRP interim data. Supplemental materials, including all source code and a synthetic example dataset, are available online.},
  number = {3},
  journaltitle = {Journal of Computational and Graphical Statistics},
  shortjournal = {Journal of Computational and Graphical Statistics},
  date = {2014-07-03},
  pages = {877-892},
  author = {Li, Fan and Baccini, Michela and Mealli, Fabrizia and Zell, Elizabeth R. and Frangakis, Constantine E. and Rubin, Donald B.},
  file = {C\:\\Users\\User\\Zotero\\storage\\Z2SXCLDC\\Li e.a. - 2014 - Multiple Imputation by Ordered Monotone Blocks Wit.pdf;C\:\\Users\\User\\Zotero\\storage\\PUZN3HTY\\10618600.2013.html}
}

@article{gelm92,
  langid = {english},
  title = {Inference from {{Iterative Simulation Using Multiple Sequences}}},
  volume = {7},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/ss/1177011136},
  abstract = {The Gibbs sampler, the algorithm of Metropolis and similar iterative simulation methods are potentially very helpful for summarizing multivariate distributions. Used naively, however, iterative simulation can give misleading answers. Our methods are simple and generally applicable to the output of any iterative simulation; they are designed for researchers primarily interested in the science underlying the data and models they are analyzing, rather than for researchers interested in the probability theory underlying the iterative simulations themselves. Our recommended strategy is to use several independent sequences, with starting points sampled from an overdispersed distribution. At each step of the iterative simulation, we obtain, for each univariate estimand of interest, a distributional estimate and an estimate of how much sharper the distributional estimate might become if the simulations were continued indefinitely. Because our focus is on applied inference for Bayesian posterior distributions in real problems, which often tend toward normality after transformations and marginalization, we derive our results as normal-theory approximations to exact Bayesian inference, conditional on the observed simulations. The methods are illustrated on a random-effects mixture model applied to experimental measurements of reaction times of normal and schizophrenic patients.},
  number = {4},
  journaltitle = {Statistical Science},
  shortjournal = {Statist. Sci.},
  date = {1992-11},
  pages = {457-472},
  keywords = {Bayesian inference,convergence of stochastic processes,ECM,EM,Gibbs sampler,importance sampling,Metropolis algorithm,multiple imputation,random-effects model,SIR},
  author = {Gelman, Andrew and Rubin, Donald B.},
  file = {C\:\\Users\\User\\Zotero\\storage\\CWG9QXQK\\Gelman en Rubin - 1992 - Inference from Iterative Simulation Using Multiple.pdf;C\:\\Users\\User\\Zotero\\storage\\EK2UCSZT\\1177011136.html}
}

@book{scha97,
  title = {Analysis of Incomplete Multivariate Data},
  isbn = {1-4398-2186-0},
  publisher = {{Chapman and Hall/CRC}},
  date = {1997},
  author = {Schafer, Joseph L}
}

@article{lace07,
  title = {Sequential Regression Multiple Imputation for Incomplete Multivariate Data Using {{Markov}} Chain {{Monte Carlo}}},
  date = {2007},
  author = {Lacerda, Miguel and Ardington, Cally and Leibbrandt, Murray},
  file = {C\:\\Users\\User\\Zotero\\storage\\NIKK7HSY\\2008_13.pdf}
}

@article{su11,
  langid = {english},
  title = {Multiple {{Imputation}} with {{Diagnostics}} (Mi) in {{R}}: {{Opening Windows}} into the {{Black Box}}},
  volume = {45},
  doi = {10.7916/D8VQ3CD3},
  shorttitle = {Multiple {{Imputation}} with {{Diagnostics}} (Mi) in {{R}}},
  abstract = {Our mi package in R has several features that allow the user to get inside the imputation process and evaluate the reasonableness of the resulting models and imputations. These features include: choice of predictors, models, and transformations for chained imputation models; standard and binned residual plots for checking the fit of the conditional distributions used for imputation; and plots for comparing the distributions of observed and imputed data. In addition, we use Bayesian models and weakly informative prior distributions to construct more stable estimates of imputation models. Our goal is to have a demonstration package that (a) avoids many of the practical problems that arise with existing multivariate imputation programs, and (b) demonstrates state-of-the-art diagnostics that can be applied more generally and can be incorporated into the software of others.},
  number = {2},
  date = {2011},
  pages = {1-31},
  author = {Su, Yu-Sung and Gelman, Andrew E. and Hill, Jennifer and Yajima, Masanao},
  file = {C\:\\Users\\User\\Zotero\\storage\\S5SZMIHW\\Su e.a. - 2011 - Multiple Imputation with Diagnostics (mi) in R Op.pdf;C\:\\Users\\User\\Zotero\\storage\\9KLRX5W9\\D8VQ3CD3.html}
}

@article{taka17,
  langid = {english},
  title = {Statistical {{Inference}} in {{Missing Data}} by {{MCMC}} and {{Non}}-{{MCMC Multiple Imputation Algorithms}}: {{Assessing}} the {{Effects}} of {{Between}}-{{Imputation Iterations}}},
  volume = {16},
  issn = {1683-1470},
  doi = {10.5334/dsj-2017-037},
  shorttitle = {Statistical {{Inference}} in {{Missing Data}} by {{MCMC}} and {{Non}}-{{MCMC Multiple Imputation Algorithms}}},
  abstract = {Incomplete data are ubiquitous in social sciences; as a consequence, available data are inefficient (ineffective) and often biased. In the literature, multiple imputation is known to be the standard method to handle missing data. While the theory of multiple imputation has been known for decades, the implementation is difficult due to the complicated nature of random draws from the posterior distribution. Thus, there are several computational algorithms in software: Data Augmentation (DA), Fully Conditional Specification (FCS), and Expectation-Maximization with Bootstrapping (EMB). Although the literature is full of comparisons between joint modeling (DA, EMB) and conditional modeling (FCS), little is known about the relative superiority between the MCMC algorithms (DA, FCS) and the non-MCMC algorithm (EMB), where MCMC stands for Markov chain Monte Carlo. Based on simulation experiments, the current study contends that EMB is a confidence proper (confidence-supporting) multiple imputation algorithm without between-imputation iterations; thus, EMB is more user-friendly than DA and FCS.},
  journaltitle = {Data Science Journal},
  date = {2017-07-28},
  pages = {37},
  keywords = {Conditional modeling,Incomplete data,Joint modeling,Markov chain Monte Carlo,MCMC,Nonresponse},
  author = {Takahashi, Masayoshi},
  file = {C\:\\Users\\User\\Zotero\\storage\\UC3SL5U6\\Takahashi - 2017 - Statistical Inference in Missing Data by MCMC and .pdf;C\:\\Users\\User\\Zotero\\storage\\UJTMEUVT\\dsj-2017-037.html}
}

@article{zhu15,
  title = {Convergence {{Properties}} of a {{Sequential Regression Multiple Imputation Algorithm}}},
  volume = {110},
  issn = {0162-1459},
  doi = {10.1080/01621459.2014.948117},
  abstract = {A sequential regression or chained equations imputation approach uses a Gibbs sampling-type iterative algorithm that imputes the missing values using a sequence of conditional regression models. It is a flexible approach for handling different types of variables and complex data structures. Many simulation studies have shown that the multiple imputation inferences based on this procedure have desirable repeated sampling properties. However, a theoretical weakness of this approach is that the specification of a set of conditional regression models may not be compatible with a joint distribution of the variables being imputed. Hence, the convergence properties of the iterative algorithm are not well understood. This article develops conditions for convergence and assesses the properties of inferences from both compatible and incompatible sequence of regression models. The results are established for the missing data pattern where each subject may be missing a value on at most one variable. The sequence of regression models are assumed to be empirically good fit for the data chosen by the imputer based on appropriate model diagnostics. The results are used to develop criteria for the choice of regression models. Supplementary materials for this article are available online.},
  number = {511},
  journaltitle = {Journal of the American Statistical Association},
  date = {2015-07-03},
  pages = {1112-1124},
  keywords = {Bayesian analysis,Chained equations,Compatible conditionals,Conditional specifications,Exponential family,Gibbs sampling,Missing data.},
  author = {Zhu, Jian and Raghunathan, Trivellore E.},
  file = {C\:\\Users\\User\\Zotero\\storage\\FPPUB4TU\\Zhu en Raghunathan - 2015 - Convergence Properties of a Sequential Regression .pdf;C\:\\Users\\User\\Zotero\\storage\\CJWQJ3FF\\01621459.2014.html}
}

@article{murr18,
  langid = {english},
  title = {Multiple {{Imputation}}: {{A Review}} of {{Practical}} and {{Theoretical Findings}}},
  volume = {33},
  issn = {0883-4237},
  doi = {10.1214/18-STS644},
  shorttitle = {Multiple {{Imputation}}},
  abstract = {Multiple imputation is a straightforward method for handling missing data in a principled fashion. This paper presents an overview of multiple imputation, including important theoretical results and their practical implications for generating and using multiple imputations. A review of strategies for generating imputations follows, including recent developments in ﬂexible joint modeling and sequential regression/chained equations/fully conditional speciﬁcation approaches. Finally, we compare and contrast diﬀerent methods for generating imputations on a range of criteria before identifying promising avenues for future research.},
  number = {2},
  journaltitle = {Statistical Science},
  shortjournal = {Statist. Sci.},
  date = {2018-05},
  pages = {142-159},
  author = {Murray, Jared S.},
  file = {C\:\\Users\\User\\Zotero\\storage\\TT9QUVIH\\Murray - 2018 - Multiple Imputation A Review of Practical and The.pdf}
}

@book{gelm13,
  location = {{Philadelphia, PA, UNITED STATES}},
  title = {Bayesian {{Data Analysis}}},
  isbn = {978-1-4398-9820-8},
  url = {http://ebookcentral.proquest.com/lib/uunl/detail.action?docID=1438153},
  publisher = {{CRC Press LLC}},
  urldate = {2019-09-12},
  date = {2013},
  keywords = {Bayesian statistical decision theory.},
  author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
  file = {C\:\\Users\\User\\Zotero\\storage\\ZSGD97ZP\\reader.html},
  note = {https://ebookcentral.proquest.com/lib/uunl/reader.action?docID=1438153\#}
}

@article{veht19,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1903.08008},
  primaryClass = {stat},
  title = {Rank-Normalization, Folding, and Localization: {{An}} Improved \$\textbackslash{}widehat\{\vphantom\}{{R}}\vphantom\{\}\$ for Assessing Convergence of {{MCMC}}},
  url = {http://arxiv.org/abs/1903.08008},
  shorttitle = {Rank-Normalization, Folding, and Localization},
  abstract = {Markov chain Monte Carlo is a key computational tool in Bayesian statistics, but it can be challenging to monitor the convergence of an iterative stochastic algorithm. In this paper we show that the convergence diagnostic \$\textbackslash{}widehat\{R\}\$ of Gelman and Rubin (1992) has serious flaws and we propose an alternative that fixes them. We also introduce a collection of quantile-based local efficiency measures, along with a practical approach for computing Monte Carlo error estimates for quantiles. We suggest that common trace plots should be replaced with rank plots from multiple chains. Finally, we give concrete recommendations for how these methods should be used in practice.},
  urldate = {2019-09-16},
  date = {2019-03-19},
  keywords = {Statistics - Methodology,Statistics - Computation},
  author = {Vehtari, Aki and Gelman, Andrew and Simpson, Daniel and Carpenter, Bob and Bürkner, Paul-Christian},
  file = {C\:\\Users\\User\\Zotero\\storage\\UY8ZSDS6\\Vehtari e.a. - 2019 - Rank-normalization, folding, and localization An .pdf;C\:\\Users\\User\\Zotero\\storage\\SZQ7NZQE\\1903.html}
}

@book{alli02,
  title = {Missing Data},
  volume = {136},
  publisher = {{Sage publications}},
  date = {2001},
  author = {Allison, Paul D.},
  file = {C\:\\Users\\User\\Zotero\\storage\\KHTDX984\\books.html}
}

@book{gilk95,
  langid = {english},
  title = {Markov {{Chain Monte Carlo}} in {{Practice}}},
  isbn = {978-0-412-05551-5},
  abstract = {In a family study of breast cancer, epidemiologists in Southern California increase the power for detecting a gene-environment interaction. In Gambia, a study helps a vaccination program reduce the incidence of Hepatitis B carriage. Archaeologists in Austria place a Bronze Age site in its true temporal location on the calendar scale. And in France, researchers map a rare disease with relatively little variation.Each of these studies applied Markov chain Monte Carlo methods to produce more accurate and inclusive results. General state-space Markov chain theory has seen several developments that have made it both more accessible and more powerful to the general statistician. Markov Chain Monte Carlo in Practice introduces MCMC methods and their applications, providing some theoretical background as well. The authors are researchers who have made key contributions in the recent development of MCMC methodology and its application. Considering the broad audience, the editors emphasize practice rather than theory, keeping the technical content to a minimum. The examples range from the simplest application, Gibbs sampling, to more complex applications. The first chapter contains enough information to allow the reader to start applying MCMC in a basic way. The following chapters cover main issues, important concepts and results, techniques for implementing MCMC, improving its performance, assessing model adequacy, choosing between models, and applications and their domains.Markov Chain Monte Carlo in Practice is a thorough, clear introduction to the methodology and applications of this simple idea with enormous potential. It shows the importance of MCMC in real applications, such as archaeology, astronomy, biostatistics, genetics, epidemiology, and image analysis, and provides an excellent base for MCMC to be applied to other fields as well.},
  pagetotal = {522},
  publisher = {{CRC Press}},
  date = {1995-12-01},
  keywords = {Mathematics / Probability & Statistics / General,Science / Life Sciences / Biology},
  author = {Gilks, W. R. and Richardson, S. and Spiegelhalter, David},
  eprinttype = {googlebooks}
}

@article{ende18,
  langid = {english},
  title = {A Fully Conditional Specification Approach to Multilevel Imputation of Categorical and Continuous Variables.},
  volume = {23},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000148},
  abstract = {Specialized imputation routines for multilevel data are widely available in software packages, but these methods are generally not equipped to handle a wide range of complexities that are typical of behavioral science data. In particular, existing imputation schemes differ in their ability to handle random slopes, categorical variables, differential relations at level-1 and level-2, and incomplete level-2 variables. Given the limitations of existing imputation tools, the purpose of this manuscript is to describe a flexible imputation approach that can accommodate a diverse set of two-level analysis problems that includes any of the aforementioned features. The procedure employs a fully conditional specification (also known as chained equations) approach with a latent variable formulation for handling incomplete categorical variables. Computer simulations suggest that the proposed procedure works quite well, with trivial biases in most cases. We provide a software program that implements the imputation strategy, and we use an artificial data set to illustrate its use.},
  number = {2},
  journaltitle = {Psychological Methods},
  shortjournal = {Psychological Methods},
  date = {2018-06},
  pages = {298-317},
  author = {Enders, Craig K. and Keller, Brian T. and Levy, Roy},
  file = {C\:\\Users\\User\\Zotero\\storage\\J2H75436\\enders-keller--levy-2017--.pdf}
}

@article{broo98,
  title = {General {{Methods}} for {{Monitoring Convergence}} of {{Iterative Simulations}}},
  volume = {7},
  issn = {1061-8600},
  doi = {10.1080/10618600.1998.10474787},
  abstract = {We generalize the method proposed by Gelman and Rubin (1992a) for monitoring the convergence of iterative simulations by comparing between and within variances of multiple chains, in order to obtain a family of tests for convergence. We review methods of inference from simulations in order to develop convergence-monitoring summaries that are relevant for the purposes for which the simulations are used. We recommend applying a battery of tests for mixing based on the comparison of inferences from individual sequences and from the mixture of sequences. Finally, we discuss multivariate analogues, for assessing convergence of several parameters simultaneously.},
  number = {4},
  journaltitle = {Journal of Computational and Graphical Statistics},
  date = {1998-12-01},
  pages = {434-455},
  keywords = {Markov chain Monte Carlo,Convergence diagnosis,Inference},
  author = {Brooks, Stephen P. and Gelman, Andrew},
  file = {C\:\\Users\\User\\Zotero\\storage\\Q7I49K25\\Brooks en Gelman - 1998 - General Methods for Monitoring Convergence of Iter.pdf;C\:\\Users\\User\\Zotero\\storage\\5Q2ITJG5\\10618600.1998.html}
}

@article{elad06,
  title = {Comparison of Methodologies to Assess the Convergence of {{Markov}} Chain {{Monte Carlo}} Methods},
  volume = {50},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2005.04.018},
  abstract = {One major challenge with the modelization of complex problems using Markov chain Monte Carlo (MCMC) methods is the determination of the length of the chain in order to reach convergence. This paper is devoted to parametric empirical methods testing the stationarity. We compare the methods of Gelman and Rubin, Yu and Mykland, Raftery and Lewis, Geweke, Riemann sums and the subsampling. These methods are tested using three examples: the simple case of the generation of a normal random variable, a bivariate mixture of normal models and a practical case taken from hydrology, namely the shifting level model. Results show that no method works in every case. We therefore suggest a joint use of these techniques. The importance of determining carefully the burn-in period is also highlighted.},
  number = {10},
  journaltitle = {Computational Statistics \& Data Analysis},
  shortjournal = {Computational Statistics \& Data Analysis},
  date = {2006-06-20},
  pages = {2685-2701},
  keywords = {Gibbs sampler,MCMC,Burn-in period,Convergence,Gelman and Rubin,Geweke,Raftery and Lewis,Riemann sums,Subsampling,Yu and Mykland},
  author = {El Adlouni, Salaheddine and Favre, Anne-Catherine and Bobée, Bernard},
  file = {C\:\\Users\\User\\Zotero\\storage\\BI6QI44D\\El Adlouni e.a. - 2006 - Comparison of methodologies to assess the converge.pdf;C\:\\Users\\User\\Zotero\\storage\\SF4UHRQ2\\S0167947305000836.html}
}

@article{schwartzPersonalityGenderAge2013,
  langid = {english},
  title = {Personality, Gender, and Age in the Language of Social Media: The Open-Vocabulary Approach},
  volume = {8},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0073791},
  shorttitle = {Personality, Gender, and Age in the Language of Social Media},
  abstract = {We analyzed 700 million words, phrases, and topic instances collected from the Facebook messages of 75,000 volunteers, who also took standard personality tests, and found striking variations in language with personality, gender, and age. In our open-vocabulary technique, the data itself drives a comprehensive exploration of language that distinguishes people, finding connections that are not captured with traditional closed-vocabulary word-category analyses. Our analyses shed new light on psychosocial processes yielding results that are face valid (e.g., subjects living in high elevations talk about the mountains), tie in with other research (e.g., neurotic people disproportionately use the phrase 'sick of' and the word 'depressed'), suggest new hypotheses (e.g., an active life implies emotional stability), and give detailed insights (males use the possessive 'my' when mentioning their 'wife' or 'girlfriend' more often than females use 'my' with 'husband' or 'boyfriend'). To date, this represents the largest study, by an order of magnitude, of language and personality.},
  number = {9},
  journaltitle = {PloS One},
  shortjournal = {PLoS ONE},
  date = {2013},
  pages = {e73791},
  keywords = {Humans,Female,Age Factors,Language,Male,Personality,Sex Factors,Social Media,Vocabulary},
  author = {Schwartz, H. Andrew and Eichstaedt, Johannes C. and Kern, Margaret L. and Dziurzynski, Lukasz and Ramones, Stephanie M. and Agrawal, Megha and Shah, Achal and Kosinski, Michal and Stillwell, David and Seligman, Martin E. P. and Ungar, Lyle H.},
  file = {C\:\\Users\\User\\Zotero\\storage\\RJ7RPFXA\\Schwartz e.a. - 2013 - Personality, gender, and age in the language of so.pdf},
  eprinttype = {pmid},
  eprint = {24086296},
  pmcid = {PMC3783449}
}

@article{sas14,
  langid = {english},
  title = {Sensitivity {{Analysis}} in {{Multiple Imputation}} for {{Missing Data}}},
  abstract = {Multiple imputation, a popular strategy for dealing with missing values, usually assumes that the data are missing at random (MAR). That is, for a variable Y, the probability that an observation is missing depends only on the observed values of other variables, not on the unobserved values of Y. It is important to examine the sensitivity of inferences to departures from the MAR assumption, because this assumption cannot be veriﬁed using the data.},
  journaltitle = {In Proceedings of the SAS Global Forum 2014 Conference},
  author = {Yuan, Yang},
  file = {C\:\\Users\\User\\Zotero\\storage\\NQKZC84X\\Yuan - Sensitivity Analysis in Multiple Imputation for Mi.pdf}
}

@article{nguy17,
  title = {Model Checking in Multiple Imputation: An Overview and Case Study},
  volume = {14},
  issn = {1742-7622},
  doi = {10.1186/s12982-017-0062-6},
  shorttitle = {Model Checking in Multiple Imputation},
  abstract = {Multiple imputation has become very popular as a general-purpose method for handling missing data. The validity of multiple-imputation-based analyses relies on the use of an appropriate model to impute the missing values. Despite the widespread use of multiple imputation, there are few guidelines available for checking imputation models.},
  number = {1},
  journaltitle = {Emerging Themes in Epidemiology},
  shortjournal = {Emerging Themes in Epidemiology},
  date = {2017-08-23},
  pages = {8},
  author = {Nguyen, Cattram D. and Carlin, John B. and Lee, Katherine J.},
  file = {C\:\\Users\\User\\Zotero\\storage\\GNUQEAQA\\Nguyen e.a. - 2017 - Model checking in multiple imputation an overview.pdf;C\:\\Users\\User\\Zotero\\storage\\CDXNKIZY\\s12982-017-0062-6.html}
}

@online{ModelCheckingMultiple,
  title = {Model Checking in Multiple Imputation: An Overview and Case Study | {{Emerging Themes}} in {{Epidemiology}} | {{Full Text}}},
  url = {https://ete-online.biomedcentral.com/articles/10.1186/s12982-017-0062-6},
  urldate = {2019-09-27},
  file = {C\:\\Users\\User\\Zotero\\storage\\W4U8K8EK\\s12982-017-0062-6.html}
}

@online{GraphicalNumericalDiagnostic,
  title = {Graphical and Numerical Diagnostic Tools to Assess Suitability of Multiple Imputations and Imputation Models - {{Bondarenko}} - 2016 - {{Statistics}} in {{Medicine}} - {{Wiley Online Library}}},
  url = {https://onlinelibrary.wiley.com/doi/full/10.1002/sim.6926},
  urldate = {2019-09-27},
  file = {C\:\\Users\\User\\Zotero\\storage\\KQX8B8SA\\sim.html}
}

@online{GraphicalNumericalDiagnostica,
  title = {Graphical and Numerical Diagnostic Tools to Assess Suitability of Multiple Imputations and Imputation Models - {{Bondarenko}} - 2016 - {{Statistics}} in {{Medicine}} - {{Wiley Online Library}}},
  url = {https://onlinelibrary.wiley.com/doi/epdf/10.1002/sim.6926},
  urldate = {2019-09-27},
  file = {C\:\\Users\\User\\Zotero\\storage\\NHQMIBN7\\sim.html}
}

@article{bond16,
  langid = {english},
  title = {Graphical and Numerical Diagnostic Tools to Assess Suitability of Multiple Imputations and Imputation Models},
  volume = {35},
  issn = {1097-0258},
  doi = {10.1002/sim.6926},
  abstract = {Multiple imputation has become a popular approach for analyzing incomplete data. Many software packages are available to multiply impute the missing values and to analyze the resulting completed data sets. However, diagnostic tools to check the validity of the imputations are limited, and the majority of the currently available methods need considerable knowledge of the imputation model. In many practical settings, however, the imputer and the analyst may be different individuals or from different organizations, and the analyst model may or may not be congenial to the model used by the imputer. This article develops and evaluates a set of graphical and numerical diagnostic tools for two practical purposes: (i) for an analyst to determine whether the imputations are reasonable under his/her model assumptions without actually knowing the imputation model assumptions; and (ii) for an imputer to fine tune the imputation model by checking the key characteristics of the observed and imputed values. The tools are based on the numerical and graphical comparisons of the distributions of the observed and imputed values conditional on the propensity of response. The methodology is illustrated using simulated data sets created under a variety of scenarios. The examples focus on continuous and binary variables, but the principles can be used to extend methods for other types of variables. Copyright © 2016 John Wiley \& Sons, Ltd.},
  number = {17},
  journaltitle = {Statistics in Medicine},
  date = {2016},
  pages = {3007-3020},
  keywords = {multiple imputation,congeniality,diagnostics,propensity score},
  author = {Bondarenko, Irina and Raghunathan, Trivellore},
  file = {C\:\\Users\\User\\Zotero\\storage\\BBYIKCGA\\Bondarenko en Raghunathan - 2016 - Graphical and numerical diagnostic tools to assess.pdf;C\:\\Users\\User\\Zotero\\storage\\Q53PV2BZ\\sim.html}
}

@article{rubin04,
  title = {The {{Design}} of a {{General}} and {{Flexible System}} for {{Handling Nonresponse}} in {{Sample Surveys}}},
  volume = {58},
  issn = {0003-1305},
  doi = {10.1198/000313004X6355},
  number = {4},
  journaltitle = {The American Statistician},
  shortjournal = {The American Statistician},
  date = {2004-11-01},
  pages = {298-302},
  author = {Rubin, Donald B},
  file = {C\:\\Users\\User\\Zotero\\storage\\RJ2HAQHD\\Rubin - 2004 - The Design of a General and Flexible System for Ha.pdf;C\:\\Users\\User\\Zotero\\storage\\EN6WGCA4\\000313004X6355.html}
}

@article{bang05,
  langid = {english},
  title = {Doubly {{Robust Estimation}} in {{Missing Data}} and {{Causal Inference Models}}},
  volume = {61},
  issn = {1541-0420},
  doi = {10.1111/j.1541-0420.2005.00377.x},
  abstract = {The goal of this article is to construct doubly robust (DR) estimators in ignorable missing data and causal inference models. In a missing data model, an estimator is DR if it remains consistent when either (but not necessarily both) a model for the missingness mechanism or a model for the distribution of the complete data is correctly specified. Because with observational data one can never be sure that either a missingness model or a complete data model is correct, perhaps the best that can be hoped for is to find a DR estimator. DR estimators, in contrast to standard likelihood-based or (nonaugmented) inverse probability-weighted estimators, give the analyst two chances, instead of only one, to make a valid inference. In a causal inference model, an estimator is DR if it remains consistent when either a model for the treatment assignment mechanism or a model for the distribution of the counterfactual data is correctly specified. Because with observational data one can never be sure that a model for the treatment assignment mechanism or a model for the counterfactual data is correct, inference based on DR estimators should improve upon previous approaches. Indeed, we present the results of simulation studies which demonstrate that the finite sample performance of DR estimators is as impressive as theory would predict. The proposed method is applied to a cardiovascular clinical trial.},
  number = {4},
  journaltitle = {Biometrics},
  date = {2005},
  pages = {962-973},
  keywords = {Missing data,Causal inference,Doubly robust estimation,Longitudinal data,Marginal structural model,Semiparametrics},
  author = {Bang, Heejung and Robins, James M.},
  file = {C\:\\Users\\User\\Zotero\\storage\\KJSVWNQL\\Bang en Robins - 2005 - Doubly Robust Estimation in Missing Data and Causa.pdf;C\:\\Users\\User\\Zotero\\storage\\MVAKGY8K\\j.1541-0420.2005.00377.html}
}

@article{meng94,
  langid = {english},
  title = {Multiple-{{Imputation Inferences}} with {{Uncongenial Sources}} of {{Input}}},
  volume = {9},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/ss/1177010269},
  abstract = {Conducting sample surveys, imputing incomplete observations, and analyzing the resulting data are three indispensable phases of modern practice with public-use data files and with many other statistical applications. Each phase inherits different input, including the information preceding it and the intellectual assessments available, and aims to provide output that is one step closer to arriving at statistical inferences with scientific relevance. However, the role of the imputation phase has often been viewed as merely providing computational convenience for users of data. Although facilitating computation is very important, such a viewpoint ignores the imputer's assessments and information inaccessible to the users. This view underlies the recent controversy over the validity of multiple-imputation inference when a procedure for analyzing multiply imputed data sets cannot be derived from (is "uncongenial" to) the model adopted for multiple imputation. Given sensible imputations and complete-data analysis procedures, inferences from standard multiple-imputation combining rules are typically superior to, and thus different from, users' incomplete-data analyses. The latter may suffer from serious nonresponse biases because such analyses often must rely on convenient but unrealistic assumptions about the nonresponse mechanism. When it is desirable to conduct inferences under models for nonresponse other than the original imputation model, a possible alternative to recreating imputations is to incorporate appropriate importance weights into the standard combining rules. These points are reviewed and explored by simple examples and general theory, from both Bayesian and frequentist perspectives, particularly from the randomization perspective. Some convenient terms are suggested for facilitating communication among researchers from different perspectives when evaluating multiple-imputation inferences with uncongenial sources of input.},
  number = {4},
  journaltitle = {Statistical Science},
  shortjournal = {Statist. Sci.},
  date = {1994-11},
  pages = {538-558},
  keywords = {importance sampling,Congeniality,incomplete data,missing data,nonresponse,normalizing constants,public-use data file,randomization,self-efficiency},
  author = {Meng, Xiao-Li},
  file = {C\:\\Users\\User\\Zotero\\storage\\PHA35LD7\\Meng - 1994 - Multiple-Imputation Inferences with Uncongenial So.pdf;C\:\\Users\\User\\Zotero\\storage\\MNNUQRPL\\1177010269.html}
}

@article{briggsMissingPresumedRandom2003,
  langid = {english},
  title = {Missing.... Presumed at Random: Cost-Analysis of Incomplete Data},
  volume = {12},
  issn = {1099-1050},
  doi = {10.1002/hec.766},
  shorttitle = {Missing.... Presumed at Random},
  abstract = {When collecting patient-level resource use data for statistical analysis, for some patients and in some categories of resource use, the required count will not be observed. Although this problem must arise in most reported economic evaluations containing patient-level data, it is rare for authors to detail how the problem was overcome. Statistical packages may default to handling missing data through a so-called ‘complete case analysis’, while some recent cost-analyses have appeared to favour an ‘available case’ approach. Both of these methods are problematic: complete case analysis is inefficient and is likely to be biased; available case analysis, by employing different numbers of observations for each resource use item, generates severe problems for standard statistical inference. Instead we explore imputation methods for generating ‘replacement’ values for missing data that will permit complete case analysis using the whole data set and we illustrate these methods using two data sets that had incomplete resource use information. Copyright © 2002 John Wiley \& Sons, Ltd.},
  number = {5},
  journaltitle = {Health Economics},
  date = {2003},
  pages = {377-392},
  keywords = {missing data,cost-analysis,economic evaluation},
  author = {Briggs, Andrew and Clark, Taane and Wolstenholme, Jane and Clarke, Philip},
  file = {C\:\\Users\\User\\Zotero\\storage\\69EUE4JH\\Briggs e.a. - 2003 - Missing.... presumed at random cost-analysis of i.pdf;C\:\\Users\\User\\Zotero\\storage\\LBI3Q8QB\\hec.html}
}

@online{ComparisonMethodologiesAssess,
  langid = {english},
  title = {(1) {{Comparison}} of {{Methodologies}} to {{Assess}} the {{Convergence}} of {{Markov Chain Monte Carlo Methods}} | {{Request PDF}}},
  url = {https://www.researchgate.net/publication/223879476_Comparison_of_Methodologies_to_Assess_the_Convergence_of_Markov_Chain_Monte_Carlo_Methods},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  journaltitle = {ResearchGate},
  urldate = {2019-10-15},
  file = {C\:\\Users\\User\\Zotero\\storage\\YMFB3DVA\\(1) Comparison of Methodologies to Assess the Conv.pdf;C\:\\Users\\User\\Zotero\\storage\\8RT5ICLI\\223879476_Comparison_of_Methodologies_to_Assess_the_Convergence_of_Markov_Chain_Monte_Carlo_Met.html}
}

@article{article,
  title = {Comparison of Methodologies to Assess the Convergence of Markov Chain Monte Carlo Methods},
  volume = {50},
  doi = {10.1016/j.csda.2005.04.018},
  journaltitle = {Computational Statistics \& Data Analysis},
  date = {2006-02},
  pages = {2685-2701},
  author = {El Adlouni, Salaheddine and Favre, Anne-Catherine and Bobée, Bernard}
}

@article{article,
  title = {Comparison of Methodologies to Assess the Convergence of Markov Chain Monte Carlo Methods},
  volume = {50},
  doi = {10.1016/j.csda.2005.04.018},
  journaltitle = {Computational Statistics \& Data Analysis},
  date = {2006-02},
  pages = {2685-2701},
  author = {El Adlouni, Salaheddine and Favre, Anne-Catherine and Bobée, Bernard}
}

@preamble{ "\ifdefined\DeclarePrefChars\DeclarePrefChars{'’-}\else\fi " }

