---
title: "Thesis Proposal"
author: "Hanne Oberman (4216318)"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  #word_document: default
  pdf_document:
    fig_caption: yes
subtitle: Methodology and Statistics for the Behavioural, Biomedical and Social Sciences
bibliography: ThesisProposal.bib
---

\centering

# ShinyMICE: an Evaluation Suite for Multiple Imputation

Supervised by prof. dr. Stef van Buuren, & dr. Gerko Vink

Department of Methodology and Statistics 

Utrecht University

Word count: 711

\raggedright 

<!-- Requirements: max 750 words (excl. references) -->

<!-- In general the proposal should focus on the relevancy of the project, the gap in the scientific literature, and the feasibility to finish the project within 8 months. Possibly specify a 'step 2' to pursue after the main objective is reached. -->


\newpage


# Introduction

At some point, any (social) scientist conducting statistical analyses will run into a missing data problem ^[@alli02]. Missingness is problematic because statistical inference cannot be performed on incomplete data, and  ad hoc solutions can yield wildly invalid results ^[@buur18]. To circumvent the *ubiquitous* problem of missing information, @rubin87 proposed the framework of multiple imputation (MI). MI is an iterative algorithmic procedure in which missing data points are 'guessed' several times. The variability between these 'guesses' validly reflects how much uncertainty in the inference is due to missing information--that is, if all statistical assumptions are met ^[@rubin87].

With MI, many assumptions are made about the nature of the observed and missing parts of the data, and their relation to the 'true' data generating model ^[@buur18]. Without proper evaluation of the imputations and the underlying assumptions, any drawn inference may erroneously be deemed valid. Such evaluation measures are currently missing or under-developed in MI software. Therefore, I aim to answer the following question: 'Which measures are vital for evaluating the validity of multiply imputed data?'. 

The goal is to develop an MI evaluation suite for the world leading R package `MICE` ^[@mice11]. This research project will aid applied researchers in drawing valid inference from incomplete data sets. Simultaneously, a contribution to the scientific literature is made by developing novel methodology and guidelines for evaluating MI data methods. 


# Literature Review

The numerous assumptions inherent to MI methods, can be roughly divided into assumptions about: 1) the nature of the missingness (*missingness mechanism*); 2)  the relation between observed and missing parts of the data (*missing data model*); and 3) the distribution of the missing values. By definition, assumptions cannot be verified on missing data. Therefore, proxy measures are used. 

The mechanism is assumed to be *missing at random*--i.e., the 'cause' of the missingness does not depend on the missing data itself ^[@rubin87]. Evaluation is done via sensitivity analyses to assess the robustness of inferences to assumption violations. Some practical guidelines exist ^[e.g., @nguy17], but current MI software does provide empirical researchers with this methodology. 

The *missing data model* assumption entails that missing values are distributed similar to observed values, after correcting for covariates ^[@buur18]. Differences between observed and imputed data can be evaluated by plotting conditional distributions ^[@abay08]. This procedure currently lacks an interactive interface and statistical tests to quantify the relations with covariates. Moreover, model fit could be assessed using *over-imputation* ^[@buur18] or *double robustness* ^[bang05].

The final assumption is met when the algorithm has converged to a stable distribution of MI values ^[@buur18]. Conventional measures to diagnose convergence--e.g., Gelman and Rubin's [-@gelm92] statistic $\widehat{R}$--do not suit multiply imputed data ^[@lace07]. Therefore, empirical researchers have to rely on a visual inspection procedure that is theoretically equivalent to $\widehat{R}$ ^[@whit11]. Not only is visually assessing convergence difficult for the untrained eye, it might also be futile. The convergence properties of MI algorithms lack scientific consensus ^[@taka17], and some default `MICE` techniques might not converge to stable distributions at all ^[@murr18]. Convergence diagnostics for MI methods have not been systematically studied ^[@buur18].


# Methods

This research project will be supervised by the `MICE` developers. I will develop novel methodology for evaluating MI data, and implement these methods using R Shiny ^[@shiny17]. The endproduct is an interactive evaluation device: '`ShinyMICE`'. The R code and documentation will be open source (available on Github). Since this project does not require the use of unpublished empirical data, I expect that the FETC will grant this project the label 'exempt'. 

The research report will consist of an investigation into algorithmic convergence of MI algorithms. I will replicate @lace07's simulation study on $\widehat{R}$, and potentially combine this convergence measure with auto-correlation or simulation error diagnostics. Ideally, I will develop a single summary indicator to flag non-convergence. 

Subsequently, I will implement this and other evaluation measures in `ShinyMICE`. The application will at least contain modules for: sensitivity analyses; data visualizations (e.g., scatter-plots, densities, cross-tabulations); and statistical evaluations of relations between variables pre- versus post-imputation (i.e., $\chi^2$-tests or t-tests).     

A working beta version of `ShinyMICE` will be considered a sufficient milestone to proceed with writing a technical paper on the methodology and the software. I aim to submit this for publication in *Journal of Statistical Software*. Finally, ShinyMICE will be integrated into the existing MICE environment, and a vignette for applied researchers will be written. 




# References

