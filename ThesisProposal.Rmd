---
title: "Thesis Proposal"
author: "Hanne Oberman (4216318)"
date: "`r Sys.Date()`"
output:
  #word_document: default
  pdf_document:
    fig_caption: yes
subtitle: Methodology and Statistics for the Behavioural, Biomedical and Social Sciences
bibliography: ThesisProposal.bib
---

\centering

# ShinyMICE: an Evaluation Suite for Multiple Imputation

Supervised by prof. dr. Stef van Buuren, & dr. Gerko Vink

Department of Methodology and Statistics 

Utrecht University

Word count: 775

\raggedright 

<!-- Requirements: max 750 words (excl. references) -->

<!-- In general the proposal should focus on the relevancy of the project, the gap in the scientific literature, and the feasibility to finish the project within 8 months. Possibly specify a 'step 2' to pursue after the main objective is reached. -->


\newpage





# Introduction

At some point, any (social) scientist conducting statistical analyses will run into a missing data problem [@alli02]. Missingness is problematic because statistical inference cannot be performed on incomplete data, and  ad hoc solutions can yield wildly invalid results [@buur18]. To circumvent the *ubiquitous* problem of missing information, @rubin87 proposed the framework of multiple imputation (MI). MI is an iterative algorithmic procedure in which missing data points are 'guessed' several times. The variability between these 'guesses' validly reflects how much uncertainty in the inference is due to missing information--that is, if all statistical assumptions are met.

With MI, many assumptions are made about the nature of the observed and missing parts of the data, and their relation to the 'true' data generating model [@buur18]. Without proper evaluation of the imputations and the underlying assumptions, any drawn inference may erroneously be deemed valid. Such evaluation measures are currently missing or under-developed in MI software. Therefore, I aim to answer the following question: 'Which measures are vital for evaluating the validity of multiply imputed data?'. 

The goal is to develop an MI evaluation suite for the world leading R package `MICE` [@mice11]. It will feature novel assessment tools (e.g., a measure to flag algorithmic non-convergence), and  interactive adaptations of (partially) implemented modules (e.g., plots to compare the incomplete and completed data sets). This research project will aid applied researchers in drawing valid inference from incomplete data sets. Simultaneously, a contribution to the scientific literature is made by developing novel methodology and guidelines for evaluating MI data methods. 

<!-- **conv te veel nadruk, weinig op diagnostiek, dat is veel sprekender.diag is ook wetenschap, meer modeling dan algorithmisch. in hoeverre duik ik de piepte om conv maat te maken. en aan de andere kant hoe grbruikers helpen om de diagn te maken in mice? bijv kijken of je overimp kunt gebruiken om te zien hoe goed het imp model is. see double robustness or sensit. analys.** -->

# Literature Review

By definition, the numerous assumptions inherent to MI methods cannot be verified from the data that is missing. 

- 1) assumption about the nature of the missingness (the missing data mechanism): that the 'cause' of the missingness is not missing, i.e. we can model the missingness from observed data [@rubin87]. Robustness of the results to violations of this assumption can be assessed using sensitivity analyses. See @nguy17 for practical guidelines. 

What is missing? Implement in MICE with guidelines to interpret. 

- 2) assumption about the missing data model: that we can 'guess' missing values based on the distribution of the observed values and covariates [@buur18]. This can be evaluated by evaluating plots of the distributions of the observed and imputed data. See review of diagnostics by @abay08. 

What is missing? To develop an interactive interface to perform analyses and plot the (in)complete data, use statistical tests to assess relations between variables, and use over-imputation or 'double robustness' to evaluate model fit. 

- 3) assumptions about the algorithm: that it has converged to a stable distribution [@buur18]. This is a gap in the scientific literature. There is no consensus on the convergence properties of MI algorithms [@taka17]. For some MI techniques it is not known whether a stable distribution can be reached at all [@murr18]. It is vital therefore, to assess convergence after running an MI algorithm. Multiply imputed data does not conform to the assumptions of conventional measures to diagnose convergence--e.g., Gelman and Rubin's [-@gelm92] statistic $\widehat{R}$ [@lace07]. Empirical researchers have to rely on a visual inspection procedure that is theoretically equivalant to $\widehat{R}$ [@whit11]. In practice, however, visually assessing convergence is subjective, and difficult for the untrained eye. 

What is missing? A systematic study of convergence diagnostics for MI methods is missing [@buur18].


<!-- Instead, imputers are  designated to evaluate the  -->
<!-- The numerous assumptions underlying MI algorithms cannot be verified from incomplete observed data (**source!**). Because the As a proxy for assumption tests on the missing part of the data,  -->

<!-- (plausibility of) multiply imputed data, and the robustness of results to the assumptions. The latter can be assessed by performing sensitivity analyses, see e.g. @nguy17 for practical guidelines. Methodology for evaluating the MI data however, is still largely missing. In the only available review of diagnostics, @abay08 overlook the most vital state to be evaluated: convergence of the algorithm. Without convergence, any 'deeper' assumption and resulting inference may be invalid. -->
<!-- <!-- The weighting approach (a selection based method) [25,26], and the pattern-mixture approach [25,27] are two methods that have been proposed in the literature for conducting sensitivity analyses within the MI framework. (https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-015-0022-1) --> -->

<!-- While the convergence properties of iterative MI algorithms are still under debate [@taka17], specific procedures like *predictive mean matching* pose entirely open questions [@murr18]. -->
<!-- <!-- "the convergence properties of FCS are currently under debate due to possible incompatibility (Li, Yu, and Rubin 2012; Zhu and Raghunathan 2015)" (@taka17, p. ) --> -->
<!-- <!-- "The convergence properties of FCS in general settings is still mostly an open question. The behavior of FCS algorithms under non- or quasi-Bayesian imputation procedures like PMM is entirely an open question" (@murr18, p.19) --> -->
<!-- @buur18 summarizes the issues with diagnosing convergence as follows: "No method works best in all circumstances. The consensus is to assess convergence with a combination of tools. The added value of using a combination of convergence diagnostics for missing data imputation has not yet been systematically studied (@buur18, \textsection 6.5). -->
<!-- <!-- Several expository reviews are available that assess convergence diagnostics for MCMC methods (Cowles and Carlin 1996; Brooks and Gelman 1998; El Adlouni, Favre, and Bobée 2006). Cowles and Carlin (1996) conclude that automated convergence monitoring (as by a machine) is unsafe and should be avoided. No method works best in all circumstances. The consensus is to assess convergence with a combination of tools. The added value of using a combination of convergence diagnostics for missing data imputation has not yet been systematically studied” (@buur18, \textsection 6.5). --> -->
<!-- Meanwhile, applied researchers have to rely on visual inspection of the algorithm's iterations [@buur18]. Convergence is said to be reached when parameters (e.g., means of completed variables) are stable across iterations [@whit11].  -->
<!-- <!-- "As MICE is an iterative procedure, it is important that convergence is achieved. This may be checked by computing, at each cycle, the means of imputed values and/or the values of regression coefficients, and seeing if they are stable" (@whit11, p. 394). --> -->
<!-- And additionally, the variation between imputation chains should be no larger than the variation within each individual chain [@buur18].  -->
<!-- <!-- “Convergence is diagnosed when the variance between different sequences is no larger than the variance within each individual sequence” (@buur18, \textsection 6.5).  --> -->
<!-- Conceptually, the visual inspection procedure is equivalent to  Gelman, & Rubin's [-@gelm92] convergence statistic $\widehat{R}$ [@li14]. In practice, however, $\widehat{R}$ cannot be applied directly to MI data, and is prone to producing false negatives [@lace07]. -->
<!-- <!-- "A common diagnostic tool is to plot one or more parameters against the iteration number and assess convergence by how different the variance between different sequences is relative to the variance within each individual sequence, similar to the Gelman-Rubin statistic (Gelman and Rubin, 1992) used in Markov chain Monte Carlo (MCMC) diagnostics" (@li14, \textsection 4.3). --> -->
<!-- The limitations in assessing convergence of MI algorithms may be overcome by combining $\widehat{R}$ with other diagnostics like auto-correlation and MC error. -->
<!-- <!-- Notwithstandingly, it seems like @su11 did implement $\widehat{R}$ as convergence criterion into their R package 'mi', including the conventional cut-off. --> -->
<!-- <!-- "R.hat: The value of the $\hat{R}$ statistic used as a convergence criterion. The default is 1.1 (Gelman and Rubin 1992; Gelman, Carlin, Stern, and Rubin 2004)" (@su11 p. 4). --> -->
<!-- <!-- "Our mi offers two ways to check the convergence of the multiple imputation procedure. By default, mi() monitors the mixing of each variable by the variance of its mean and standard deviation within and between different chains of the imputation. If the $\hat{R}$ statistic is smaller than 1.1, (i.e., the difference of the within and between variance is trivial), the imputation is considered converged (Gelman, Carlin, Stern, and Rubin 2004). Additionally, by specifying mi(data, check.coef.convergence = TRUE, ...), users can check the convergence of the parameters of the conditional models" (@su11, p. 13). --> -->
<!-- <!-- So it is worth investigating the validity of this convergence statistic in the context of multiple imputation.  --> -->

<!-- **Add: which other assumptions could be checked? The assumption of MAR. Ideally, we would want to study all possible combinations of variables: univariate, bivariate, etc. And to include both plots and stats.** -->


# Methods

This research project will be supervised by the `MICE` developers. I will develop novel methodology for evaluating MI data, and implement these methods using R Shiny [@shiny17]. The endproduct is an interactive evaluation device: '`ShinyMICE`'. The R code and documentation will be open source (available on Github). Since this project does not require the use of unpublished empirical data, I expect that the FETC will grant this project the label 'exempt'. 

<!-- Develop a valid method to investigate the plausibility of multiply imputed data based on: -->

<!-- - models (imputation and non-response) -->

<!-- - data features (cross-tabs, point estimates, aggregate statistics, etc.) -->

<!-- - assumptions -->

The research report will consist of an investigation into algorithmic convergence of MI algorithms. Ideally, this will result in a single summary indicator to flag non-convergence (potentially based on $\widehat{R}$, autocorrelation, or simulation error).

In the second stage of the research project I will focus on other evaluation measures to implement in `ShinyMICE`. The application will at least consist of the following: 1) one or more measures to assess algorithmic convergence; 2) data visualizations (e.g., scatter-plots, densities, and cross-tabulations of the data pre- and post-imputation); and 3) statistical evaluation of relations between variables pre- versus post-imputation (i.e., $\chi^2$-tests or t-tests). 
<!-- 1) comparing the imputed data to the observations, 2) inspecting the algorithmic convergence, 3) inspecting multivariate distributions, 4) the plausibility of the imputed data -->

A working beta version of ShinyMICE will be considered a sufficient milestone to proceed with writing a technical paper on the methodology and the software. I aim to submit this for publication in *Journal of Statistical Software*. Finally, ShinyMICE will be integrated into the existing MICE environment, and a vignette for applied researchers will be written. 




# References

