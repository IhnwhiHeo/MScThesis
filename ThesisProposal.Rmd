---
title: "Thesis Proposal"
author: "Hanne Oberman (4216318)"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  #word_document: 
  pdf_document: 
    fig_caption: yes
subtitle: Methodology and Statistics for the Behavioural, Biomedical and Social Sciences
bibliography: ThesisProposal.bib
csl: acm.csl

---

\centering

# ShinyMICE: an Evaluation Suite for Multiple Imputation

Supervised by prof. dr. Stef van Buuren, & dr. Gerko Vink

Department of Methodology and Statistics 

Utrecht University

Word count: 713


\raggedright 

<!-- Requirements: max 750 words (excl. references) -->

<!-- In general the proposal should focus on the relevancy of the project, the gap in the scientific literature, and the feasibility to finish the project within 8 months. Possibly specify a 'step 2' to pursue after the main objective is reached. -->


\newpage


# Introduction

At some point, any scientist conducting statistical analyses will run into a missing data problem [@alli02]. Missingness is problematic because statistical inference cannot be performed on incomplete data, and  ad hoc solutions can yield wildly invalid results [@buur18]. To circumvent the ubiquitous problem of missing information, Rubin [@rubin87] proposed the framework of multiple imputation (MI). MI is an iterative algorithmic procedure in which missing data points are 'guessed' (i.e. imputed) several times. The variability between the imputations validly reflects how much uncertainty in the inference is due to missing information--that is, if all statistical assumptions are met [@rubin87].

With MI, many assumptions are made about the nature of the observed and missing parts of the data and their relation to the 'true' data generating model [@buur18]. Without proper evaluation of the imputations and the underlying assumptions, any drawn inference may erroneously be deemed valid. Such evaluation measures are currently missing or under-developed in MI software, like the world leading `R` package `MICE` [@mice11]. Therefore, I will answer the following question: 'Which measures are vital for evaluating the validity of multiply imputed data?'. 

# Literature Review

The validity of the MI solution depends on numerous assumptions that cannot be verified from the observed data alone. Consequently, existing evaluation methods rely on **[rephrase: proxy measures]**. For the following assumptions, no reliable *proxy measures* have been proposed and/or implemented: 1) *ignorability* of the *missingness mechanism*; 2) congruity of the *imputation models*; and 3) *compatibility* of the MI modeling procedure. 
<!-- Rubin (1987b, 160-66) subdivided the work needed to create imputations into three tasks. The modeling task chooses a specific model for the data, the estimation task formulates the posterior parameters distribution given the model and the imputation task takes a random draws for the missing data by drawing successively from parameter and data distributions @buur18 par 4.5.1 -->

<!-- account for the process that created the missing data, -->
<!-- preserve the relations in the data, and -->
<!-- preserve the uncertainty about these relations. -->


<!-- The modeling task chooses a specific model for the data -> assumes ignorability. -->

<!-- The estimation task computes the posterior distribution of the model parameters of the assumes correct specification of the model. ->

<!-- The imputation task draws plausible values from the conditional posterior distribution of the missing data given the estimated parameters -> assumes convergence in distribution -->


1. A missingness mechanism is said to be ignorable when the sprobability to be missing does not depend on the missing data itself [@rubin87]. Evaluation of this assumption is usually done with sensitivity analyses to assess the robustness of inferences to violations. Some practical guidelines exist (e.g., [@nguy17]), but current MI software does not facilitate this methodology for empirical researchers. 


<!-- **miss mech is model hoe obs and miss samenhangen, draait om ignorability =  op moment dat je obs kunt gebruiken om miss data te schatten. hoeft niet hetzelfde te zijn. relatie kunnen we uit de data halen, ondanks dat we de ontbrekende waaren zelf kunnen negeren, alleen het mech dat die waarden heeft gegenereerd. ** -->

2. Congruent imputation models capture all required relations between observed and missing parts of the data. The exent to which this has been successful can be evaluated by plotting conditional distributions [@abay08]. Such visualizations are available in `MICE`, but subsequent statistical tests to quantify the relations with covariates are not provided. Additionally, there is potential to assess model fit by means of *over-imputation* [@buur18] or *double robustness* [@bang05]. 

<!-- **er zijn 3 modellen: analyse, imputatie, non-response model (of missing data model). in mice wordt iedere var voorspeld uit andere vars. eerst analysemodel, is scientific mod of int. als je dat niet meeneemt dan mis je ruimte in alg om datgene dat je probeert te vinden mee te nemen (non-comp). non-resp is verklaren waarom missings in een var vaker voorkomen dan andere vars. meng's 1994 paper on compatibility and congeneality.** -->

3. The third assumption is met when the MI algorithm converges to a stable distribution. However, conventional measures to diagnose convergence-- e.g., Gelman and Rubin's [-@gelm92] statistic $\widehat{R}$ --are not applicable on multiply imputed data [@lace07]. Therefore, empirical researchers have to rely on visual inspection procedures that are theoretically equivalent to $\widehat{R}$ [@whit11]. Visually assessing convergence is not only difficult to the untrained eye, it might also be futile. The convergence properties of MI algorithms lack scientific consensus [@taka17], and some default `MICE` techniques might not converge to stable distributions at all [@murr18]. Moreover, convergence diagnostics for MI methods have not been systematically studied [@buur18].

In short, the existing literature provides both possibilities and limitations to evaluating the validity of multiply imputed data. The goal of this research project is to develop novel methodology and guidelines for evaluating MI methods, and implement these in an interactive evaluation framework for multiple imputation. This framework will aid applied researchers in drawing valid inference from incomplete datasets. 


# Approach

Initially, the research project will consist of an investigation into algorithmic convergence of MI algorithms. I will replicate Lacerda et al.'s simulation study on $\widehat{R}$ [@lace07], and develop novel guidelines for assessing convergence. Ideally, I will integrate several diagnostics (e.g., $\widehat{R}$, *auto-correlation*, and *simulation error*) into a single summary indicator to flag non-convergence. 

Subsequently, I will use `R Shiny` [@shiny17] to implement the convergence indicator and other evaluation measures in `ShinyMICE`, see Figure 1. The application will at least contain methodology for: sensitivity analyses; data visualizations (e.g., scatter-plots, densities, cross-tabulations); and statistical evaluations of relations between variables pre- versus post-imputation (i.e., $\chi^2$-tests or $t$-tests).     

![Preliminary impression of the interactive `ShinyMICE` user interface.](Figures/Impression.png)

A working beta version of `ShinyMICE` will be considered a sufficient milestone to proceed with writing a technical paper on the methodology and the software. I will submit the paper for publication in *Journal of Statistical Software*. Finally, `ShinyMICE` will be integrated into the existing `MICE` environment, and a vignette for applied researchers will be written. 

The R code and documentation of this project will be open source (available on Github). Since the study does not require the use of unpublished empirical data, I expect that the FETC will grant the label 'exempt'. 



# References

