---
title: "Literature on non-convergence"
author: "Hanne Oberman"
date: "4-3-2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

"There are two general approaches for imputing the data multiply: joint modelling (JM) and fully conditional specification (FCS). Different techniques for MI using JM have been developed in the literature.1,10-13 This approach is useful when the data can be assumed to have a well-known multivariate distribution, such as the multivariate normal distribution. A variety of JM imputation algorithms for longitudinal data and multilevel missing data are also available in the literature.14-18 The FCS approach is much more flexible but potentially flawed by an incompatible distribution and does not require a joint multivariate distribution, but it specifies an imputation model for each missing variable using its fully conditional density. Fully means that conditioning is usually done on all other variables in the dataset. It is therefore also useful and can be implemented with ease for complex data structures. The idea of FCS has been implemented in the literature with different names: stochastic relaxation by Kennickell,19 variable-by-variable imputation by Brand,20 regression switching by van Buuren et al.,21 sequential regression by Raghunathan et al.,22 ordered pseudo-Gibbs sampler by Heckerman et al.,23 partially incompatible Markov chain Monte Carlo (MCMC) by Rubin,24 iterative univariate imputation by Gelman,25 MI by chained equations (MICE) by van Buuren and Oudshoorn26, van Buuren and Groothuis-Oudshoorn27 and White et al.7 and FCS by van Buuren.28 "


"As MICE is an iterative procedure, it is important that convergence is achieved. This may be checked by computing, at
each cycle, the means of imputed values and/or the values of regression coefficients, and seeing if they are stable. For the
example in Section 9, these values appeared stable from the very first cycle. We have never found 10 cycles inadequate,
but larger numbers of cycles might in principle be required when incomplete variables are very strongly associated" (@whit11, p. 394).

# For ShinyMICE
"Determining How Many Imputations Are Needed
Working back from the computation of statistics and confidence intervals, we need
to ask how many imputations are required to generate useful statistical inferences. Surprisingly, the answer is often no more than five and sometimes as little as two or three.
The relative efficiency (RE) of the m imputations given a fraction of missing data
(Rubin, 1987, p. 114) is computed in standard error units as
...
Even when Î³ is .9, five imputations give a relative efficiency of .92. Consequently,
one rarely has to create more than five imputations, and 10 imputations are more than
suitable in almost any realistic application" (https://journals.sagepub.com/doi/pdf/10.1177/1094428103255532, p. 291).

"Multiple imputation (MI) is the method of choice for many incomplete data problems. MI incorporates the uncertainty about the missing data by creating $m>2$ imputed data sets. Missing values are filled in under an imputation model. The imputed data that result from the imputation model is then analyzed by the analysis model. Separate analyses can be combined to get a single inference or set of estimates by making use of the combining rules derived by Rubin (1987)." 

"Missing data problems are ubiquitous in observational data and common among social science applications. Statistical inference that does not adequately account for the missing data is widely known to lead to biased results and inflated (or deflated) variance estimates (King et al. 2001; Molenberghs et al. 2014; Rubin 1976; White and Carlin 2010). Even though most statistical software platforms provide methods that adequately handle missing data (the most popular of these is multiple imputations [MI]), they are often ignored by applied researchers."

"Statistical methods to deal with missing data have greatly increased in both number and sophistication over the past 20 years. Multiple imputation (MI) is now considered the leading method for dealing with item-missing data that are missing completely at random (MCAR) or missing at random (MAR; Rubin 1987, 1996; Schafer 1997). Because standard statistical software packages now have built-in routines for implementing MI, its usage is making its way into the analyses of ordinary (but statistically savvy) researchers. Research emphasizes the importance of including in the imputation model all variables from the intended analytic model as well as variables correlated with the potential mechanisms of missingness and variables correlated with the variables to be imputed (hereafter referred to as auxiliary variables) as they improve the capability of the model to predict the missing values (Collins, Schafer, and Kam 2001; Schafer 2003)".

"Multiple imputation (MI) is a popular method for repairing and analyzing data with missing values (Rubin 1987). Using MI, you fill in the missing values with imputed values that are drawn at random from a posterior predictive distribution that is conditioned on the observed values Yobs. You repeat the process of imputation M times to obtain M imputed data sets, which you analyze as though they were complete. You then combine the results of the M analyses to obtain a point estimate ...".